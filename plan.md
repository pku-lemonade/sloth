*假如我是设计者，我需要关注哪些问题:*  
我期待找到每条链路的路由流量，这个主要是方便统计瓶颈在哪里  
我还希望监视资源的调度使用，主要是为了看是什么资源限制了进步  
我是不是应该考虑什么导致的failslow？对failslow进行监视  
我还要看每个操作对于每个核时间的占用(当然是分布式)  


*热点分析:*
重要的是找到潜在的可能造成问题的热点(最好能回溯到根因)
首先send和recv之间是一定要建立连接的，这是为了监视是否可能链路拥堵造成程序性能降低(规定所有的send,recv节点都不能约化)
但是为了贴合模拟，首先要将程序划分为block，考虑再block内部，对于不在意的冗余信息进行简化
对于scalana中的简化程序图(
    \  /
     \/    对于一个节点，如果它的父节点都不能被约化，它就不能被约化?但是目前来看似乎没这个必要简化，因为trigger_idx比较小
)
我觉得，其实分析这个程序，最重要的是:
对于每个操作，记录它的could_exe,以及exe(这是对于所有的都需要的)
对于每个被triggered的操作，记录它的begin_trigger到end_trigger
对于关键路径的操作，需要消除同一时刻多个命令的竞争

*套用scalana:*
scalana给我的感觉更像是它知道热点在哪里，然后往这个方向去凑:?
scalana采用了一种计时的机制，利用PAPI在一定的时间段执行一次中断，每次中断的时候记录函数的递归调用栈
然后用add2line对应到实际的函数+行数
但是它这个算法的核心实际上是采样,记录此时在哪个函数中执行，后面通过读取函数LLVM生成的结构图来定位
那么其实也可以把many-core的进行同样的采样,也就是在每个env的每个clock进行采样，
但是这样相对于原来的程序，时间要暴增，采样时间还是要稍微拉长
记录此时在哪些事件上进行的等待，然后最后统计造成等待的指令的百分比(极有可能是SEND,RECV),通过这个定位热点的位置
问题在于对于这个等待的细化:
1.等待是否需要回溯:有时候等待可能不是被等待的位置的函数的问题，可能它本身也是在等待?这还可能有多个依赖，这个时候怎么办？
2.对于依赖多个指令的指令:从第一条依赖的指令到达启动作为开始等待,还是从还在等最后一条依赖的指令到达的时候开启等待?
3.对于依赖RECV的指令:又应该怎么处理？
4.对于计算指令的等待又应该怎么算?(目前想法是时间切片，这个时间切片存在的必要条件是竞争的指令数>N)
*单独统计块调度造成的性能损失:*
维护一个readylist，每次没有ready就加1，来体现块调度造成的性能损失(这个需要单独统计计算百分比，主要需要找到哪个块调度造成的性能损失最大,就是这个耽误的时间独立来算,从而找到块调度的热点)
*SCALANA扩展问题*
SCALANA的python部分记录了扩展到多核，然后再统计每个核在每个node消耗的时间，但是这里有个问题:SCALANA记录的是一个node，然后将执行程序在不同的核上进行计算，利用看它采样上消耗的总执行时间进行回归，然后找到离群值作为异常顶点
但是问题在于manycore似乎没有这种经常被调用的函数或者很大的循环，一个命令就执行一次，如果以block为单位，统计block的话，似乎说的过去(如果扩展的多核也是用的同样的block)?
*SCALANA没有实现root cause backtrace*
*基于上面问题，我目前的对于利用SCALANA想法:*
首先采样的周期要稍微长一些:待定?
对于每条指令，将它还差最后一条指令的时候作为问题
然后是对于热点的分析:
对于多条指令且只剩一条维护一个ready_list，记录所有ready的指令?(ready_list过长可能会爆炸)
每次采样的时候，找到ready_list->往回找指令，这个如果找的层数少的话(eg:1层)，感觉需要在PSG图上进行聚类,如果找的层数多的话,要进行分类讨论以及更深的位置应该怎么找?(如果一直都只有一个前驱倒还好，找到正在算的指令/等待的RECV，但是多个前驱就难了)



如果是以blocksize为单位,******************************************



但是感觉都这样采样了，不如直接用一种分析的方式:
对于block导致的估计完全可以用ready_time(这里的ready真的是所有指令都准备好了)，和一个block_start,利用block_start-ready_time作为block_size导致时延的估计




但是对于单个指令:可以用一种可能收益来计算，对于每个由多个指令触发的指令，计算倒数第二条指令触发的事件和最后一条指令触发的时间,这样的话，在触发图上，每个指令的只有一个入口,通过这些记录的信息进行图运算也很容易，但是这样的话困难在于怎么识别一个指令最后造成的影响?




约定:PSG每个指令的唯一标识:position,id(哪个PE的哪个id)=>node,这些都在architecture层进行分析，在PSG进行回溯


想到了一个改进,如果启用profile，跑两次，第一次形成一个last_trigger_graph,第二次跑利用last_trigger_graph来进行分析(分析正在等待,这样避免了在直接分析上指数爆炸的问题)
基于一个假设:这个调度很好，分布也很均匀,但是有些函数
定义的depth相当于层数,对应于看一定层数还有多少指令相关联

